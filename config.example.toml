# LocalGPT Configuration
# Copy to ~/.localgpt/config.toml

[agent]
# Default model to use for chat
# Use "claude-cli/opus", "claude-cli/sonnet", or "claude-cli/haiku" for Claude CLI
default_model = "claude-cli/opus"

# Context window size (in tokens)
context_window = 128000

# Reserve tokens for response
reserve_tokens = 8000

# OpenAI configuration
[providers.openai]
api_key = "${OPENAI_API_KEY}"
base_url = "https://api.openai.com/v1"

# Anthropic configuration (optional)
# [providers.anthropic]
# api_key = "${ANTHROPIC_API_KEY}"
# base_url = "https://api.anthropic.com"

# Ollama configuration (for local models)
# [providers.ollama]
# endpoint = "http://localhost:11434"
# model = "llama3"

# Claude CLI configuration (uses local claude CLI command)
# Requires claude CLI to be installed: https://github.com/anthropics/claude-code
[providers.claude_cli]
command = "claude"
model = "opus"  # opus, sonnet, or haiku

[heartbeat]
# Enable automatic heartbeat
enabled = true

# How often to check HEARTBEAT.md
interval = "30m"

# Only run during these hours (optional)
# active_hours = { start = "09:00", end = "22:00" }

# Timezone for active hours
# timezone = "America/Los_Angeles"

[memory]
# Where to store memory files
workspace = "~/.localgpt/workspace"

# Embedding model for semantic search (future)
embedding_model = "text-embedding-3-small"

# Chunk size for indexing (tokens)
chunk_size = 400

# Overlap between chunks (tokens)
chunk_overlap = 80

[server]
# Enable HTTP server
enabled = true

# Port to listen on
port = 18790

# Bind address (127.0.0.1 for localhost only)
bind = "127.0.0.1"

[logging]
# Log level: trace, debug, info, warn, error
level = "info"

# Log file path
file = "~/.localgpt/logs/agent.log"
