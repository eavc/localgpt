use anyhow::{anyhow, Result};
use rusqlite::{params, Connection};
use sha2::{Digest, Sha256};
use std::fs;
use std::path::{Path, PathBuf};
use std::sync::{Arc, Mutex};
use std::time::Duration;
use tracing::{debug, info};
use uuid::Uuid;

use super::embeddings::{cosine_similarity, deserialize_embedding, serialize_embedding};
use super::search::MemoryChunk;

#[derive(Clone)]
pub struct MemoryIndex {
    conn: Arc<Mutex<Connection>>,
    workspace: PathBuf,
    db_path: PathBuf,
}

#[derive(Debug)]
pub struct ReindexStats {
    pub files_processed: usize,
    pub files_updated: usize,
    pub chunks_indexed: usize,
    pub duration: Duration,
}

impl MemoryIndex {
    /// Create a new memory index with database at the specified path
    pub fn new_with_db_path(workspace: &Path, db_path: &Path) -> Result<Self> {
        // Ensure parent directory exists
        if let Some(parent) = db_path.parent() {
            fs::create_dir_all(parent)?;
        }

        let conn = Connection::open(db_path)?;

        // Check if we need to migrate from old schema
        let needs_migration = Self::needs_schema_migration(&conn)?;
        if needs_migration {
            info!("Migrating database schema to OpenClaw-compatible format");
            Self::migrate_to_openclaw_schema(&conn)?;
        }

        // Initialize OpenClaw-compatible schema
        conn.execute_batch(
            r#"
            -- Metadata key/value store
            CREATE TABLE IF NOT EXISTS meta (
                key TEXT PRIMARY KEY,
                value TEXT NOT NULL
            );

            -- File tracking (OpenClaw-compatible)
            CREATE TABLE IF NOT EXISTS files (
                path TEXT PRIMARY KEY,
                source TEXT NOT NULL DEFAULT 'memory',
                hash TEXT NOT NULL,
                mtime INTEGER NOT NULL,
                size INTEGER NOT NULL
            );

            -- Chunked content (OpenClaw-compatible)
            CREATE TABLE IF NOT EXISTS chunks (
                id TEXT PRIMARY KEY,
                path TEXT NOT NULL,
                source TEXT NOT NULL DEFAULT 'memory',
                start_line INTEGER NOT NULL,
                end_line INTEGER NOT NULL,
                hash TEXT NOT NULL,
                model TEXT NOT NULL DEFAULT '',
                text TEXT NOT NULL,
                embedding TEXT NOT NULL DEFAULT '',
                updated_at INTEGER NOT NULL
            );

            -- Embedding cache (OpenClaw-compatible)
            CREATE TABLE IF NOT EXISTS embedding_cache (
                provider TEXT NOT NULL,
                model TEXT NOT NULL,
                provider_key TEXT NOT NULL,
                hash TEXT NOT NULL,
                embedding TEXT NOT NULL,
                dims INTEGER,
                updated_at INTEGER NOT NULL,
                PRIMARY KEY (provider, model, provider_key, hash)
            );

            -- Indexes
            CREATE INDEX IF NOT EXISTS idx_chunks_path ON chunks(path);
            CREATE INDEX IF NOT EXISTS idx_chunks_source ON chunks(source);
            CREATE INDEX IF NOT EXISTS idx_embedding_cache_updated_at ON embedding_cache(updated_at);
            "#,
        )?;

        // Create FTS5 table (OpenClaw-compatible with UNINDEXED columns)
        Self::ensure_fts_table(&conn)?;

        // Ensure source column exists on older tables
        Self::ensure_column(&conn, "files", "source", "TEXT NOT NULL DEFAULT 'memory'")?;
        Self::ensure_column(&conn, "chunks", "source", "TEXT NOT NULL DEFAULT 'memory'")?;

        Ok(Self {
            conn: Arc::new(Mutex::new(conn)),
            workspace: workspace.to_path_buf(),
            db_path: db_path.to_path_buf(),
        })
    }

    /// Create a new memory index with database in workspace (legacy path)
    pub fn new(workspace: &Path) -> Result<Self> {
        let db_path = workspace.join("memory.sqlite");
        Self::new_with_db_path(workspace, &db_path)
    }

    /// Index a file, returning true if it was updated
    pub fn index_file(&self, path: &Path, force: bool) -> Result<bool> {
        let content = fs::read_to_string(path)?;
        let file_hash = hash_content(&content);
        let metadata = fs::metadata(path)?;
        let mtime = metadata
            .modified()?
            .duration_since(std::time::UNIX_EPOCH)?
            .as_secs() as i64;
        let size = metadata.len() as i64;

        let relative_path = path
            .strip_prefix(&self.workspace)
            .unwrap_or(path)
            .to_string_lossy()
            .to_string();

        let conn = self
            .conn
            .lock()
            .map_err(|e| anyhow!("Lock poisoned: {}", e))?;

        // Check if file has changed
        if !force {
            let existing: Option<String> = conn
                .query_row(
                    "SELECT hash FROM files WHERE path = ?1",
                    params![&relative_path],
                    |row| row.get(0),
                )
                .ok();

            if existing.as_deref() == Some(&file_hash) {
                debug!("File unchanged, skipping: {}", relative_path);
                return Ok(false);
            }
        }

        debug!("Indexing file: {}", relative_path);

        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)?
            .as_secs() as i64;

        // Update file record (OpenClaw-compatible columns)
        conn.execute(
            "INSERT OR REPLACE INTO files (path, source, hash, mtime, size) VALUES (?1, 'memory', ?2, ?3, ?4)",
            params![&relative_path, &file_hash, mtime, size],
        )?;

        // Delete existing chunks and their FTS entries
        Self::delete_chunks_for_path(&conn, &relative_path)?;

        // Create new chunks (OpenClaw-compatible)
        let chunks = chunk_text(&content, 400, 80);

        for chunk in chunks.iter() {
            let chunk_id = Uuid::new_v4().to_string();
            let chunk_hash = hash_content(&chunk.content);

            conn.execute(
                r#"INSERT INTO chunks (id, path, source, start_line, end_line, hash, model, text, embedding, updated_at)
                   VALUES (?1, ?2, 'memory', ?3, ?4, ?5, '', ?6, '', ?7)"#,
                params![&chunk_id, &relative_path, chunk.line_start, chunk.line_end, &chunk_hash, &chunk.content, now],
            )?;

            // Insert into FTS
            Self::insert_fts(&conn, &chunk_id, &relative_path, "memory", "", chunk.line_start, chunk.line_end, &chunk.content)?;
        }

        Ok(true)
    }

    /// Delete chunks for a path and their FTS entries
    fn delete_chunks_for_path(conn: &Connection, path: &str) -> Result<()> {
        // Delete from FTS first (get chunk IDs)
        let mut stmt = conn.prepare("SELECT id FROM chunks WHERE path = ?1")?;
        let chunk_ids: Vec<String> = stmt
            .query_map(params![path], |row| row.get(0))?
            .filter_map(|r| r.ok())
            .collect();

        for chunk_id in chunk_ids {
            let _ = conn.execute(
                "DELETE FROM chunks_fts WHERE id = ?1",
                params![&chunk_id],
            );
        }

        // Delete chunks
        conn.execute("DELETE FROM chunks WHERE path = ?1", params![path])?;
        Ok(())
    }

    /// Insert into FTS table
    fn insert_fts(
        conn: &Connection,
        id: &str,
        path: &str,
        source: &str,
        model: &str,
        start_line: i32,
        end_line: i32,
        text: &str,
    ) -> Result<()> {
        let _ = conn.execute(
            "INSERT INTO chunks_fts (text, id, path, source, model, start_line, end_line) VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7)",
            params![text, id, path, source, model, start_line, end_line],
        );
        Ok(())
    }

    /// Search using FTS5
    pub fn search(&self, query: &str, limit: usize) -> Result<Vec<MemoryChunk>> {
        // Escape special FTS5 characters
        let escaped_query = escape_fts_query(query);

        let conn = self
            .conn
            .lock()
            .map_err(|e| anyhow!("Lock poisoned: {}", e))?;

        // OpenClaw-compatible: use 'path', 'start_line', 'end_line', 'text' columns
        let mut stmt = conn.prepare(
            r#"
            SELECT fts.path, fts.start_line, fts.end_line, fts.text, bm25(chunks_fts) as score
            FROM chunks_fts fts
            WHERE chunks_fts MATCH ?1
            ORDER BY score
            LIMIT ?2
            "#,
        )?;

        let rows = stmt.query_map(params![&escaped_query, limit as i64], |row| {
            Ok(MemoryChunk {
                file: row.get(0)?,
                line_start: row.get(1)?,
                line_end: row.get(2)?,
                content: row.get(3)?,
                score: row.get::<_, f64>(4)?.abs(), // BM25 returns negative scores
            })
        })?;

        let mut results = Vec::new();
        for row in rows {
            results.push(row?);
        }

        Ok(results)
    }

    /// Get total chunk count
    pub fn chunk_count(&self) -> Result<usize> {
        let conn = self
            .conn
            .lock()
            .map_err(|e| anyhow!("Lock poisoned: {}", e))?;
        let count: i64 = conn.query_row("SELECT COUNT(*) FROM chunks", [], |row| row.get(0))?;
        Ok(count as usize)
    }

    /// Get chunk count for a specific file
    pub fn file_chunk_count(&self, path: &Path) -> Result<usize> {
        let relative_path = path
            .strip_prefix(&self.workspace)
            .unwrap_or(path)
            .to_string_lossy()
            .to_string();

        let conn = self
            .conn
            .lock()
            .map_err(|e| anyhow!("Lock poisoned: {}", e))?;
        let count: i64 = conn.query_row(
            "SELECT COUNT(*) FROM chunks WHERE path = ?1",
            params![&relative_path],
            |row| row.get(0),
        )?;
        Ok(count as usize)
    }

    /// Get database size in bytes
    pub fn size_bytes(&self) -> Result<u64> {
        if self.db_path.exists() {
            Ok(fs::metadata(&self.db_path)?.len())
        } else {
            Ok(0)
        }
    }

    /// Get the database path
    pub fn db_path(&self) -> &Path {
        &self.db_path
    }

    /// Check if we need to migrate from old LocalGPT schema to OpenClaw schema
    fn needs_schema_migration(conn: &Connection) -> Result<bool> {
        // Check for old schema indicators:
        // 1. chunks table has 'file_path' column instead of 'path'
        // 2. chunks table has 'content' column instead of 'text'
        // 3. chunks.id is INTEGER instead of TEXT
        let result: rusqlite::Result<String> =
            conn.query_row("PRAGMA table_info(chunks)", [], |row| row.get(1));

        if result.is_err() {
            // chunks table doesn't exist yet, no migration needed
            return Ok(false);
        }

        // Check column names
        let has_file_path: bool = conn
            .prepare("SELECT file_path FROM chunks LIMIT 0")
            .is_ok();
        let has_content: bool = conn.prepare("SELECT content FROM chunks LIMIT 0").is_ok();

        // Old schema has file_path and content columns
        Ok(has_file_path || has_content)
    }

    /// Migrate from old LocalGPT schema to OpenClaw-compatible schema
    fn migrate_to_openclaw_schema(conn: &Connection) -> Result<()> {
        // Start transaction
        conn.execute("BEGIN TRANSACTION", [])?;

        // 1. Rename old tables
        let _ = conn.execute("ALTER TABLE chunks RENAME TO chunks_old", []);
        let _ = conn.execute("ALTER TABLE files RENAME TO files_old", []);

        // 2. Drop old FTS and triggers
        let _ = conn.execute("DROP TABLE IF EXISTS chunks_fts", []);
        let _ = conn.execute("DROP TRIGGER IF EXISTS chunks_ai", []);
        let _ = conn.execute("DROP TRIGGER IF EXISTS chunks_ad", []);
        let _ = conn.execute("DROP TRIGGER IF EXISTS chunks_au", []);

        // 3. Create new tables with OpenClaw schema
        conn.execute(
            r#"
            CREATE TABLE IF NOT EXISTS files (
                path TEXT PRIMARY KEY,
                source TEXT NOT NULL DEFAULT 'memory',
                hash TEXT NOT NULL,
                mtime INTEGER NOT NULL,
                size INTEGER NOT NULL
            )
            "#,
            [],
        )?;

        conn.execute(
            r#"
            CREATE TABLE IF NOT EXISTS chunks (
                id TEXT PRIMARY KEY,
                path TEXT NOT NULL,
                source TEXT NOT NULL DEFAULT 'memory',
                start_line INTEGER NOT NULL,
                end_line INTEGER NOT NULL,
                hash TEXT NOT NULL,
                model TEXT NOT NULL DEFAULT '',
                text TEXT NOT NULL,
                embedding TEXT NOT NULL DEFAULT '',
                updated_at INTEGER NOT NULL
            )
            "#,
            [],
        )?;

        // 4. Migrate data from old tables
        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap_or_default()
            .as_secs() as i64;

        // Migrate files
        let _ = conn.execute(
            r#"
            INSERT INTO files (path, source, hash, mtime, size)
            SELECT path, 'memory', hash, mtime, size FROM files_old
            "#,
            [],
        );

        // Migrate chunks - generate new TEXT UUIDs for each row
        // Check if old schema has embedding columns
        let has_embedding_cols = conn
            .prepare("SELECT embedding FROM chunks_old LIMIT 0")
            .is_ok();

        // Read old data and insert with new UUIDs
        if has_embedding_cols {
            // Old schema has embedding columns
            let mut stmt = conn.prepare(
                "SELECT file_path, line_start, line_end, content, embedding, embedding_model FROM chunks_old",
            )?;
            let rows = stmt.query_map([], |row| {
                Ok((
                    row.get::<_, String>(0)?,
                    row.get::<_, i32>(1)?,
                    row.get::<_, i32>(2)?,
                    row.get::<_, String>(3)?,
                    row.get::<_, Option<String>>(4)?,
                    row.get::<_, Option<String>>(5)?,
                ))
            })?;

            for row in rows {
                let (file_path, line_start, line_end, content, embedding, model) = row?;
                let new_id = Uuid::new_v4().to_string();
                let hash = hash_content(&content);
                let model = model.unwrap_or_default();
                let embedding = embedding.unwrap_or_default();

                conn.execute(
                    r#"
                    INSERT INTO chunks (id, path, source, start_line, end_line, hash, model, text, embedding, updated_at)
                    VALUES (?1, ?2, 'memory', ?3, ?4, ?5, ?6, ?7, ?8, ?9)
                    "#,
                    params![&new_id, &file_path, line_start, line_end, &hash, &model, &content, &embedding, now],
                )?;
            }
        } else {
            // Old schema without embedding columns
            let mut stmt = conn.prepare(
                "SELECT file_path, line_start, line_end, content FROM chunks_old",
            )?;
            let rows = stmt.query_map([], |row| {
                Ok((
                    row.get::<_, String>(0)?,
                    row.get::<_, i32>(1)?,
                    row.get::<_, i32>(2)?,
                    row.get::<_, String>(3)?,
                ))
            })?;

            for row in rows {
                let (file_path, line_start, line_end, content) = row?;
                let new_id = Uuid::new_v4().to_string();
                let hash = hash_content(&content);

                conn.execute(
                    r#"
                    INSERT INTO chunks (id, path, source, start_line, end_line, hash, model, text, embedding, updated_at)
                    VALUES (?1, ?2, 'memory', ?3, ?4, ?5, '', ?6, '', ?7)
                    "#,
                    params![&new_id, &file_path, line_start, line_end, &hash, &content, now],
                )?;
            }
        }

        // 5. Drop old tables
        let _ = conn.execute("DROP TABLE IF EXISTS chunks_old", []);
        let _ = conn.execute("DROP TABLE IF EXISTS files_old", []);

        conn.execute("COMMIT", [])?;
        info!("Schema migration completed successfully");
        Ok(())
    }

    /// Create FTS5 table with OpenClaw-compatible structure
    fn ensure_fts_table(conn: &Connection) -> Result<()> {
        // OpenClaw uses UNINDEXED columns for metadata
        let result = conn.execute(
            r#"
            CREATE VIRTUAL TABLE IF NOT EXISTS chunks_fts USING fts5(
                text,
                id UNINDEXED,
                path UNINDEXED,
                source UNINDEXED,
                model UNINDEXED,
                start_line UNINDEXED,
                end_line UNINDEXED
            )
            "#,
            [],
        );

        match result {
            Ok(_) => debug!("FTS5 table created/verified"),
            Err(e) => debug!("FTS5 table creation skipped: {}", e),
        }

        Ok(())
    }

    /// Ensure a column exists on a table (for migrations)
    fn ensure_column(conn: &Connection, table: &str, column: &str, definition: &str) -> Result<()> {
        let sql = format!("SELECT {} FROM {} LIMIT 0", column, table);
        if conn.prepare(&sql).is_err() {
            let alter = format!("ALTER TABLE {} ADD COLUMN {} {}", table, column, definition);
            conn.execute(&alter, [])?;
            debug!("Added column {} to table {}", column, table);
        }
        Ok(())
    }

    /// Get chunks that need embeddings (OpenClaw-compatible: id is TEXT, text column)
    pub fn chunks_without_embeddings(&self, limit: usize) -> Result<Vec<(String, String)>> {
        let conn = self
            .conn
            .lock()
            .map_err(|e| anyhow!("Lock poisoned: {}", e))?;

        let mut stmt = conn.prepare(
            "SELECT id, text FROM chunks WHERE embedding = '' OR embedding IS NULL LIMIT ?1",
        )?;

        let rows = stmt.query_map(params![limit as i64], |row| {
            Ok((row.get::<_, String>(0)?, row.get::<_, String>(1)?))
        })?;

        let mut results = Vec::new();
        for row in rows {
            results.push(row?);
        }

        Ok(results)
    }

    /// Store embedding for a chunk (OpenClaw-compatible: id is TEXT, model column)
    pub fn store_embedding(&self, chunk_id: &str, embedding: &[f32], model: &str) -> Result<()> {
        let conn = self
            .conn
            .lock()
            .map_err(|e| anyhow!("Lock poisoned: {}", e))?;

        let embedding_json = serialize_embedding(embedding);
        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)?
            .as_secs() as i64;

        conn.execute(
            "UPDATE chunks SET embedding = ?1, model = ?2, updated_at = ?3 WHERE id = ?4",
            params![&embedding_json, model, now, chunk_id],
        )?;

        Ok(())
    }

    /// Vector search using embeddings (OpenClaw-compatible columns)
    pub fn search_vector(
        &self,
        query_embedding: &[f32],
        model: &str,
        limit: usize,
    ) -> Result<Vec<MemoryChunk>> {
        let conn = self
            .conn
            .lock()
            .map_err(|e| anyhow!("Lock poisoned: {}", e))?;

        // Get all chunks with embeddings for this model
        let mut stmt = conn.prepare(
            "SELECT id, path, start_line, end_line, text, embedding
             FROM chunks
             WHERE embedding != '' AND embedding IS NOT NULL AND model = ?1",
        )?;

        let rows = stmt.query_map(params![model], |row| {
            Ok((
                row.get::<_, String>(0)?,
                row.get::<_, String>(1)?,
                row.get::<_, i32>(2)?,
                row.get::<_, i32>(3)?,
                row.get::<_, String>(4)?,
                row.get::<_, String>(5)?,
            ))
        })?;

        // Compute similarities and sort
        let mut scored: Vec<(f32, MemoryChunk)> = Vec::new();

        for row in rows {
            let (_, path, start_line, end_line, text, embedding_json) = row?;
            let embedding = deserialize_embedding(&embedding_json);

            if embedding.len() == query_embedding.len() {
                let similarity = cosine_similarity(query_embedding, &embedding);
                scored.push((
                    similarity,
                    MemoryChunk {
                        file: path,
                        line_start: start_line,
                        line_end: end_line,
                        content: text,
                        score: similarity as f64,
                    },
                ));
            }
        }

        // Sort by similarity (descending)
        scored.sort_by(|a, b| b.0.partial_cmp(&a.0).unwrap_or(std::cmp::Ordering::Equal));

        // Take top results
        Ok(scored.into_iter().take(limit).map(|(_, chunk)| chunk).collect())
    }

    /// Hybrid search: combine FTS and vector results
    pub fn search_hybrid(
        &self,
        query: &str,
        query_embedding: Option<&[f32]>,
        model: &str,
        limit: usize,
        text_weight: f32,
        vector_weight: f32,
    ) -> Result<Vec<MemoryChunk>> {
        // Get FTS results
        let fts_results = self.search(query, limit * 2)?;

        // Get vector results if embedding provided
        let vector_results = if let Some(embedding) = query_embedding {
            self.search_vector(embedding, model, limit * 2)?
        } else {
            Vec::new()
        };

        // Merge results using weighted scores
        let mut merged: std::collections::HashMap<String, (f32, MemoryChunk)> =
            std::collections::HashMap::new();

        // Add FTS results (normalize BM25 score to 0-1 range)
        let max_fts_score = fts_results
            .iter()
            .map(|r| r.score)
            .fold(0.0f64, |a, b| a.max(b));
        let max_fts_score = if max_fts_score > 0.0 { max_fts_score } else { 1.0 };

        for result in fts_results {
            let key = format!("{}:{}:{}", result.file, result.line_start, result.line_end);
            let normalized_score = (result.score / max_fts_score) as f32;
            let weighted_score = normalized_score * text_weight;
            merged.insert(key, (weighted_score, result));
        }

        // Add/merge vector results
        for result in vector_results {
            let key = format!("{}:{}:{}", result.file, result.line_start, result.line_end);
            let weighted_score = result.score as f32 * vector_weight;

            if let Some((existing_score, existing_chunk)) = merged.get_mut(&key) {
                *existing_score += weighted_score;
                existing_chunk.score = *existing_score as f64;
            } else {
                let mut chunk = result;
                chunk.score = weighted_score as f64;
                merged.insert(key, (weighted_score, chunk));
            }
        }

        // Sort by combined score and take top results
        let mut results: Vec<_> = merged.into_values().collect();
        results.sort_by(|a, b| b.0.partial_cmp(&a.0).unwrap_or(std::cmp::Ordering::Equal));

        Ok(results.into_iter().take(limit).map(|(_, chunk)| chunk).collect())
    }

    /// Count chunks with embeddings (OpenClaw-compatible: model column)
    pub fn embedded_chunk_count(&self, model: &str) -> Result<usize> {
        let conn = self
            .conn
            .lock()
            .map_err(|e| anyhow!("Lock poisoned: {}", e))?;

        let count: i64 = conn.query_row(
            "SELECT COUNT(*) FROM chunks WHERE embedding != '' AND embedding IS NOT NULL AND model = ?1",
            params![model],
            |row| row.get(0),
        )?;

        Ok(count as usize)
    }
}

fn hash_content(content: &str) -> String {
    let mut hasher = Sha256::new();
    hasher.update(content.as_bytes());
    format!("{:x}", hasher.finalize())
}

fn escape_fts_query(query: &str) -> String {
    // Wrap in quotes to treat as phrase, escape internal quotes
    let escaped = query.replace('"', "\"\"");
    format!("\"{}\"", escaped)
}

struct ChunkInfo {
    line_start: i32,
    line_end: i32,
    content: String,
}

fn chunk_text(text: &str, target_tokens: usize, overlap_tokens: usize) -> Vec<ChunkInfo> {
    let lines: Vec<&str> = text.lines().collect();
    let mut chunks = Vec::new();

    if lines.is_empty() {
        return chunks;
    }

    // Rough estimate: 4 chars per token
    let target_chars = target_tokens * 4;
    let overlap_chars = overlap_tokens * 4;

    let mut start_line = 0;
    let mut current_chars = 0;
    let mut chunk_lines = Vec::new();

    for (i, line) in lines.iter().enumerate() {
        chunk_lines.push(*line);
        current_chars += line.len() + 1; // +1 for newline

        if current_chars >= target_chars || i == lines.len() - 1 {
            // Create chunk
            chunks.push(ChunkInfo {
                line_start: (start_line + 1) as i32,
                line_end: (i + 1) as i32,
                content: chunk_lines.join("\n"),
            });

            // Calculate overlap for next chunk
            let mut overlap_len = 0;
            let mut overlap_start = chunk_lines.len();

            for (j, line) in chunk_lines.iter().enumerate().rev() {
                overlap_len += line.len() + 1;
                if overlap_len >= overlap_chars {
                    overlap_start = j;
                    break;
                }
            }

            // Prepare for next chunk
            if overlap_start < chunk_lines.len() {
                start_line = start_line + overlap_start;
                chunk_lines = chunk_lines[overlap_start..].to_vec();
                current_chars = chunk_lines.iter().map(|l| l.len() + 1).sum();
            } else {
                start_line = i + 1;
                chunk_lines.clear();
                current_chars = 0;
            }
        }
    }

    chunks
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;

    #[test]
    fn test_chunk_text() {
        let text = "Line 1\nLine 2\nLine 3\nLine 4\nLine 5";
        let chunks = chunk_text(text, 10, 2); // Small chunks for testing

        assert!(!chunks.is_empty());
        assert_eq!(chunks[0].line_start, 1);
    }

    #[test]
    fn test_memory_index() -> Result<()> {
        let temp_dir = TempDir::new()?;
        let workspace = temp_dir.path();

        // Create a test file
        let test_file = workspace.join("test.md");
        fs::write(
            &test_file,
            "# Test\n\nThis is a test document.\n\nWith multiple lines.",
        )?;

        let index = MemoryIndex::new(workspace)?;
        index.index_file(&test_file, false)?;

        assert!(index.chunk_count()? > 0);

        let results = index.search("test document", 10)?;
        assert!(!results.is_empty());

        Ok(())
    }
}
